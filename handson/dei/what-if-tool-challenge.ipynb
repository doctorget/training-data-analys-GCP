{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bTry4ZMD2859"
   },
   "source": [
    "# Welcome to the What-If Tool Challenge Lab!\n",
    "\n",
    "In this notebook, you will use mortgage data from NY in 2017 to create two binary classifiers to determine if a mortgage applicant will be granted a loan.\n",
    "\n",
    "You will train a classifier on two datasets. One will be trained on the complete dataset, and the other will be trained on a subset of the dataset, where 90% of the female applicants that were granted a loan were removed from the training data (so the dataset has 90% less females that were granted loans).\n",
    "\n",
    "You will then compare and examine the two models using the What-If Tool."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zU9bzX-VWQCb"
   },
   "source": [
    "# Download and import the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nhmYvLmUxSqU"
   },
   "outputs": [],
   "source": [
    "# First, let's get some modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import collections\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.utils import shuffle\n",
    "from witwidget.notebook.visualization import WitWidget, WitConfigBuilder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "oVhFQBvggsio",
    "outputId": "bb218967-74a5-4fdf-fd5c-87e8c1143872"
   },
   "outputs": [],
   "source": [
    "# Now we will download some data from the Consumer Finance public datasets\n",
    "# You can find it here: https://www.consumerfinance.gov/data-research/hmda/historic-data/?geo=ny&records=all-records&field_descriptions=labels\n",
    "\n",
    "!wget https://files.consumerfinance.gov/hmda-historic-loan-data/hmda_2017_ny_all-records_labels.zip\n",
    "!unzip hmda_2017_ny_all-records_labels.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uFyKHeHZD1e6"
   },
   "source": [
    "# Process the Data\n",
    "\n",
    "In this section, you **don't need to write any code**. We suggest you read through the cells to understand how the dataset is processed.\n",
    "\n",
    "Here, we first import that dataset into a Pandas dataframe. Then we process the data to exlude incomplete information and make a simple binary classification of loan approvals. We then create two datasets, one complete and one where 90% of female applicants are removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LSsrdPdyCVYn"
   },
   "outputs": [],
   "source": [
    "# Set column dtypes for Pandas\n",
    "column_names = collections.OrderedDict({\n",
    "  'as_of_year': np.int16,\n",
    "  'agency_abbr': 'category',\n",
    "  'loan_type': 'category',\n",
    "  'property_type': 'category',\n",
    "  'loan_purpose': 'category',\n",
    "  'owner_occupancy': np.int8,\n",
    "  'loan_amt_000s': np.float64,\n",
    "  'preapproval': 'category',\n",
    "  'county_code': np.float64,\n",
    "  'applicant_income_00s': np.float64,\n",
    "  'purchaser_type': 'category',\n",
    "  'hoepa_status': 'category',\n",
    "  'lien_status': 'category',\n",
    "  'population': np.float64,\n",
    "  'ffiec_median_fam_income': np.float64,\n",
    "  'tract_to_msamd_income': np.float64,\n",
    "  'num_of_owner_occupied_units': np.float64,\n",
    "  'number_of_1_to_4_family_units': np.float64,\n",
    "  'approved': np.int8, \n",
    "  'applicant_race_name_3': 'category',\n",
    "  'applicant_race_name_4': 'category',\n",
    "  'applicant_race_name_5': 'category',\n",
    "  'co_applicant_race_name_3': 'category',\n",
    "  'co_applicant_race_name_4': 'category',\n",
    "  'co_applicant_race_name_5': 'category'\n",
    "})\n",
    "\n",
    "# Import the CSV into a dataframe\n",
    "data = pd.read_csv('hmda_2017_ny_all-records_labels.csv', dtype=column_names)\n",
    "data = shuffle(data, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "qWNJwq2-Htxz"
   },
   "outputs": [],
   "source": [
    "# Only use a subset of the columns for these models\n",
    "text_columns_to_keep = [\n",
    "             'agency_name',\n",
    "             'loan_type_name',\n",
    "             'property_type_name',\n",
    "             'loan_purpose_name',\n",
    "             'owner_occupancy_name',\n",
    "             'applicant_ethnicity_name',\n",
    "             'applicant_race_name_1',\n",
    "             'applicant_sex_name',                      \n",
    "]\n",
    "numeric_columns_to_keep = [\n",
    "             'loan_amount_000s',\n",
    "             'applicant_income_000s',\n",
    "             'population',\n",
    "             'minority_population',\n",
    "             'hud_median_family_income' \n",
    "]\n",
    "\n",
    "columns_to_keep = text_columns_to_keep + numeric_columns_to_keep + ['action_taken_name']\n",
    "\n",
    "# Drop columns with incomplete information and drop columns that don't have loan orignated or denied, to make this a simple binary classification\n",
    "df = data[columns_to_keep].dropna()\n",
    "binary_df = df[df.action_taken_name.isin(['Loan originated', 'Application denied by financial institution'])].copy()\n",
    "binary_df.loc[:,'loan_granted'] = np.where(binary_df['action_taken_name'] == 'Loan originated', 1, 0)\n",
    "binary_df = binary_df.drop(columns=['action_taken_name'])\n",
    "\n",
    "# Drop 90% of loaned female applicants for a \"bad training data\" version\n",
    "loaned_females = (binary_df['applicant_sex_name'] == 'Female') & (binary_df['loan_granted'] == 1)\n",
    "bad_binary_df = binary_df.drop(binary_df[loaned_females].sample(frac=.9).index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "ic6mWTvENrLd",
    "outputId": "b06712db-bae2-464a-e9b8-16df8312f60f"
   },
   "outputs": [],
   "source": [
    "# Now lets' see the distribution of approved / denied classes (0: denied, 1: approved)\n",
    "print(binary_df['loan_granted'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "6h3kQmIqMLYr"
   },
   "outputs": [],
   "source": [
    "# Turn categorical string features into simple 0/1 features (like turning \"sex\" into \"sex_male\" and \"sex_female\")\n",
    "dummies_df = pd.get_dummies(binary_df, columns=text_columns_to_keep)\n",
    "dummies_df = dummies_df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "bad_dummies_df = pd.get_dummies(bad_binary_df, columns=text_columns_to_keep)\n",
    "bad_dummies_df = bad_dummies_df.sample(frac=1).reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3VfdY4PzWOoI"
   },
   "outputs": [],
   "source": [
    "# Normalize the numeric columns so that they all have the same scale to simplify modeling/training\n",
    "def normalize():\n",
    "  min_max_scaler = preprocessing.MinMaxScaler()\n",
    "  column_names_to_normalize = ['loan_amount_000s', 'applicant_income_000s', 'minority_population', 'hud_median_family_income', 'population']\n",
    "  x = dummies_df[column_names_to_normalize].values\n",
    "  x_scaled = min_max_scaler.fit_transform(x)\n",
    "  df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = dummies_df.index)\n",
    "  dummies_df[column_names_to_normalize] = df_temp\n",
    "\n",
    "  x = bad_dummies_df[column_names_to_normalize].values\n",
    "  x_scaled = min_max_scaler.fit_transform(x)\n",
    "  bad_df_temp = pd.DataFrame(x_scaled, columns=column_names_to_normalize, index = bad_dummies_df.index)\n",
    "  bad_dummies_df[column_names_to_normalize] = bad_df_temp\n",
    "\n",
    "normalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Np8JM4KINnKC"
   },
   "outputs": [],
   "source": [
    "# Get the training data & labels\n",
    "test_data_with_labels = dummies_df\n",
    "\n",
    "train_data = dummies_df\n",
    "train_labels = train_data['loan_granted']\n",
    "train_data = train_data.drop(columns=['loan_granted'])\n",
    "\n",
    "# Get the bad training data and labels\n",
    "bad_train_data = bad_dummies_df\n",
    "bad_train_labels = bad_train_data['loan_granted']\n",
    "bad_train_data = bad_dummies_df.drop(columns=['loan_granted'])\n",
    "\n",
    "# Split the data into train / test sets for Model 1\n",
    "x,y = train_data,train_labels\n",
    "train_data,test_data,train_labels,test_labels = train_test_split(x,y)\n",
    "\n",
    "# Split the bad data into train / test sets for Model 2\n",
    "bad_x,bad_y=bad_train_data,bad_train_labels\n",
    "bad_train_data,bad_test_data,bad_train_labels,bad_test_labels = train_test_split(bad_x,bad_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "MyUxXszu0Mp0"
   },
   "source": [
    "# Train your Models\n",
    "\n",
    "In this section, you will write some code to train two Keras models. You can view the documentation [here](https://www.tensorflow.org/api_docs/python/tf/keras/Sequential)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "K685pKOMUPQD"
   },
   "source": [
    "## Train your first model on the complete dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "PvgHgZ-agsi_",
    "outputId": "dfebe31d-48d2-4864-f412-2e2eb2f63823"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import initializers\n",
    "from tensorflow.keras import optimizers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "\n",
    "# This is the size of the array we'll be feeding into our model for each example\n",
    "input_size = len(train_data.iloc[0])\n",
    "\n",
    "# Train the first model on the complete dataset\n",
    "\n",
    "\n",
    "# ---- TODO ---------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 105
    },
    "colab_type": "code",
    "id": "CWGEGaxPgsjD",
    "outputId": "ecd88ac8-27e2-4449-81d2-3345b3859295"
   },
   "outputs": [],
   "source": [
    "# Save your model\n",
    "!mkdir -p saved_model\n",
    "model.save('saved_model/my_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "hg0bnNVwgsjF",
    "outputId": "7229df46-835c-4d39-b5ad-4b7ede50e459"
   },
   "outputs": [],
   "source": [
    "# Get predictions on the test set and print the accuracy score (Model 1)\n",
    "y_pred = model.predict(test_data)\n",
    "acc = accuracy_score(test_labels, y_pred.round())\n",
    "print(\"Model 1 Accuracy: %.2f%%\" % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "U2hPhuA-UXTT"
   },
   "source": [
    "## Train your second model on the limited datset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "NP8cr7JvgsjH",
    "outputId": "3ce7e913-97a7-4999-ea33-fd7b64c039ea"
   },
   "outputs": [],
   "source": [
    "# This is the size of the array we'll be feeding into our model for each example\n",
    "input_size = len(train_data.iloc[0])\n",
    "\n",
    "# Train your second model on the limited dataset\n",
    "\n",
    "\n",
    "# ---- TODO ---------\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "5UauXNlMgsjK",
    "outputId": "bee7c208-662b-409a-8cbc-ac447df67cec"
   },
   "outputs": [],
   "source": [
    "# Save your model\n",
    "!mkdir -p saved_bad_model\n",
    "bad_model.save('saved_bad_model/my_bad_model') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 102
    },
    "colab_type": "code",
    "id": "n0UxiCcygsjM",
    "outputId": "82a69023-ac14-4414-dd20-ed57eb2a6e09"
   },
   "outputs": [],
   "source": [
    "# Get predictions on the test set and print the accuracy score (Model 2)\n",
    "bad_y_pred = bad_model.predict(bad_test_data)\n",
    "acc = accuracy_score(bad_test_labels, bad_y_pred.round())\n",
    "print(\"Model 2 Accuracy: %.2f%%\" % (acc * 100.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-5X33HRf0b2C"
   },
   "source": [
    "# Deploy your models to the AI Platform\n",
    "\n",
    "In this section, you will first need to create a Cloud Storage bucket to store your models, then you will use gcloud commands to copy them over.\n",
    "\n",
    "You will then create two AI Platform models and their associated versions.\n",
    "\n",
    "\n",
    "You can find the documentation for this [here](https://cloud.google.com/ai-platform/prediction/docs/deploying-models#gcloud_1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Jfp8H0esC6k_"
   },
   "outputs": [],
   "source": [
    "# ---- TODO ---------\n",
    "\n",
    "# Fill out this information:\n",
    "\n",
    "GCP_PROJECT = '#TODO'\n",
    "MODEL_BUCKET = 'gs:// #TODO'\n",
    "MODEL_NAME = 'complete_model'\n",
    "BAD_MODEL_NAME = 'limited_model'\n",
    "VERSION_NAME = 'v1'\n",
    "REGION = 'us-central1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "NJOTCAsLDjcF"
   },
   "outputs": [],
   "source": [
    "# Copy your model files to Cloud Storage\n",
    "!gsutil cp -r ./saved_model $MODEL_BUCKET\n",
    "!gsutil cp -r ./saved_bad_model $MODEL_BUCKET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dbGP-3qIDoza"
   },
   "outputs": [],
   "source": [
    "# Configure gcloud to use your project\n",
    "!gcloud config set project $GCP_PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BSfwEaE8DpOP"
   },
   "outputs": [],
   "source": [
    "# Create an AI Platform model for your complete model\n",
    "\n",
    "# ---- TODO ---------\n",
    "\n",
    "\n",
    "\n",
    "# Now create a version, this will take ~2 minutes to deploy. You should use runtime-version=1.14.\n",
    "\n",
    "\n",
    "\n",
    "# ---- TODO ---------\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "xuPue_4Mgsjd"
   },
   "outputs": [],
   "source": [
    "# Create an AI Platform model for your limited model\n",
    "\n",
    "\n",
    "# ---- TODO ---------\n",
    "\n",
    "\n",
    "\n",
    "# Now create a version, this will take ~2 minutes to deploy. You should use runtime-version=1.14.\n",
    "\n",
    "\n",
    "\n",
    "# ---- TODO ---------\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4IZAJ1LrqUha"
   },
   "source": [
    "# Using the What-if Tool to interpret your model\n",
    "Once your models have deployed, you're now ready to connect them to the What-if Tool using the WitWidget. \n",
    "\n",
    "We've provided the Config Builder code and a couple of functions to get the class predictions from the models, which are necessary inputs for the WIT. If you've successfully deployed and saved your models, **you won't need to modify any code in this cell**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 817,
     "resources": {
      "http://localhost:8080/data/plugins_listing": {
       "data": "CjwhRE9DVFlQRSBodG1sPgo8aHRtbCBsYW5nPWVuPgogIDxtZXRhIGNoYXJzZXQ9dXRmLTg+CiAgPG1ldGEgbmFtZT12aWV3cG9ydCBjb250ZW50PSJpbml0aWFsLXNjYWxlPTEsIG1pbmltdW0tc2NhbGU9MSwgd2lkdGg9ZGV2aWNlLXdpZHRoIj4KICA8dGl0bGU+RXJyb3IgNDA0IChOb3QgRm91bmQpISExPC90aXRsZT4KICA8c3R5bGU+CiAgICAqe21hcmdpbjowO3BhZGRpbmc6MH1odG1sLGNvZGV7Zm9udDoxNXB4LzIycHggYXJpYWwsc2Fucy1zZXJpZn1odG1se2JhY2tncm91bmQ6I2ZmZjtjb2xvcjojMjIyO3BhZGRpbmc6MTVweH1ib2R5e21hcmdpbjo3JSBhdXRvIDA7bWF4LXdpZHRoOjM5MHB4O21pbi1oZWlnaHQ6MTgwcHg7cGFkZGluZzozMHB4IDAgMTVweH0qID4gYm9keXtiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9lcnJvcnMvcm9ib3QucG5nKSAxMDAlIDVweCBuby1yZXBlYXQ7cGFkZGluZy1yaWdodDoyMDVweH1we21hcmdpbjoxMXB4IDAgMjJweDtvdmVyZmxvdzpoaWRkZW59aW5ze2NvbG9yOiM3Nzc7dGV4dC1kZWNvcmF0aW9uOm5vbmV9YSBpbWd7Ym9yZGVyOjB9QG1lZGlhIHNjcmVlbiBhbmQgKG1heC13aWR0aDo3NzJweCl7Ym9keXtiYWNrZ3JvdW5kOm5vbmU7bWFyZ2luLXRvcDowO21heC13aWR0aDpub25lO3BhZGRpbmctcmlnaHQ6MH19I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LnBuZykgbm8tcmVwZWF0O21hcmdpbi1sZWZ0Oi01cHh9QG1lZGlhIG9ubHkgc2NyZWVuIGFuZCAobWluLXJlc29sdXRpb246MTkyZHBpKXsjbG9nb3tiYWNrZ3JvdW5kOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSBuby1yZXBlYXQgMCUgMCUvMTAwJSAxMDAlOy1tb3otYm9yZGVyLWltYWdlOnVybCgvL3d3dy5nb29nbGUuY29tL2ltYWdlcy9sb2dvcy9lcnJvcnBhZ2UvZXJyb3JfbG9nby0xNTB4NTQtMngucG5nKSAwfX1AbWVkaWEgb25seSBzY3JlZW4gYW5kICgtd2Via2l0LW1pbi1kZXZpY2UtcGl4ZWwtcmF0aW86Mil7I2xvZ297YmFja2dyb3VuZDp1cmwoLy93d3cuZ29vZ2xlLmNvbS9pbWFnZXMvbG9nb3MvZXJyb3JwYWdlL2Vycm9yX2xvZ28tMTUweDU0LTJ4LnBuZykgbm8tcmVwZWF0Oy13ZWJraXQtYmFja2dyb3VuZC1zaXplOjEwMCUgMTAwJX19I2xvZ297ZGlzcGxheTppbmxpbmUtYmxvY2s7aGVpZ2h0OjU0cHg7d2lkdGg6MTUwcHh9CiAgPC9zdHlsZT4KICA8YSBocmVmPS8vd3d3Lmdvb2dsZS5jb20vPjxzcGFuIGlkPWxvZ28gYXJpYS1sYWJlbD1Hb29nbGU+PC9zcGFuPjwvYT4KICA8cD48Yj40MDQuPC9iPiA8aW5zPlRoYXTigJlzIGFuIGVycm9yLjwvaW5zPgogIDxwPiAgPGlucz5UaGF04oCZcyBhbGwgd2Uga25vdy48L2lucz4K",
       "headers": [
        [
         "content-length",
         "1449"
        ],
        [
         "content-type",
         "text/html; charset=utf-8"
        ]
       ],
       "ok": false,
       "status": 404,
       "status_text": ""
      }
     }
    },
    "colab_type": "code",
    "id": "bQrAb7lbOhvI",
    "outputId": "4ac675b9-a80c-4e93-de5b-a8cdf825dea4"
   },
   "outputs": [],
   "source": [
    "#@title Show model results in WIT\n",
    "num_datapoints = 1000  #@param {type: \"number\"}\n",
    "\n",
    "# Column indices to strip out from data from WIT before passing it to the model.\n",
    "columns_not_for_model_input = [\n",
    "    test_data_with_labels.columns.get_loc('loan_granted'),\n",
    "]\n",
    "\n",
    "# Return model predictions.\n",
    "def custom_predict(examples_to_infer):\n",
    "  # Delete columns not used by model\n",
    "  model_inputs = np.delete(\n",
    "      np.array(examples_to_infer), columns_not_for_model_input, axis=1).tolist()\n",
    "  # Get the class predictions from the model.\n",
    "  preds = model.predict(model_inputs)\n",
    "  preds = [[1 - pred[0], pred[0]] for pred in preds]\n",
    "  return preds\n",
    "  \n",
    "# Return 'bad' model predictions.\n",
    "def bad_custom_predict(examples_to_infer):\n",
    "  # Delete columns not used by model\n",
    "  model_inputs = np.delete(\n",
    "      np.array(examples_to_infer), columns_not_for_model_input, axis=1).tolist()\n",
    "  # Get the class predictions from the model.\n",
    "  preds = bad_model.predict(model_inputs)\n",
    "  preds = [[1 - pred[0], pred[0]] for pred in preds]\n",
    "  return preds\n",
    "\n",
    "examples_for_wit = test_data_with_labels.values.tolist()\n",
    "column_names = test_data_with_labels.columns.tolist()\n",
    "\n",
    "config_builder = (WitConfigBuilder(\n",
    "    examples_for_wit[:num_datapoints],feature_names=column_names)\n",
    "    .set_custom_predict_fn(bad_custom_predict)\n",
    "    .set_target_feature('loan_granted')\n",
    "    .set_label_vocab(['denied', 'accepted'])\n",
    "    .set_compare_custom_predict_fn(custom_predict)\n",
    "    .set_model_name('limited')\n",
    "    .set_compare_model_name('complete'))\n",
    "WitWidget(config_builder, height=800)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "-5X33HRf0b2C"
   ],
   "machine_shape": "hm",
   "name": "What-If Tool Challenge Lab",
   "provenance": []
  },
  "environment": {
   "name": "tf2-gpu.2-1.m46",
   "type": "gcloud",
   "uri": "gcr.io/deeplearning-platform-release/tf2-gpu.2-1:m46"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
